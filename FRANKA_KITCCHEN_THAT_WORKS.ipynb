{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU25HhvBdlh78aRqIxNQYG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anna4142/miniBET/blob/main/FRANKA_KITCCHEN_THAT_WORKS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G4QG9_Us4pxz",
        "outputId": "d0fe8123-5a62-4e72-d88e-4a7064ae59cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.24.1\n",
            "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/696.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m430.1/696.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m696.3/696.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m696.4/696.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.24.1) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.24.1) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.24.1) (0.0.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.24.1-py3-none-any.whl size=793130 sha256=9b1cdab4d1ed4e422b1bd475d124d878a41bf487bb53c7d74871713e541b1f9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/fb/19/388995b88cb551717a8dff40c889172cd12fadf994216a0a22\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.24.1\n",
            "Collecting mujoco==2.3.3\n",
            "  Downloading mujoco-2.3.3-2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco==2.3.3) (1.4.0)\n",
            "Collecting glfw (from mujoco==2.3.3)\n",
            "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mujoco==2.3.3) (1.26.4)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco==2.3.3) (3.1.7)\n",
            "Downloading mujoco-2.3.3-2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.8.0 mujoco-2.3.3\n",
            "Collecting d4rl\n",
            "  Downloading d4rl-1.1-py3-none-any.whl.metadata (270 bytes)\n",
            "Requirement already satisfied: gym<0.25.0 in /usr/local/lib/python3.10/dist-packages (from d4rl) (0.24.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from d4rl) (1.26.4)\n",
            "Collecting mujoco-py (from d4rl)\n",
            "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl.metadata (669 bytes)\n",
            "Collecting pybullet (from d4rl)\n",
            "  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from d4rl) (3.12.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from d4rl) (2.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from d4rl) (8.1.7)\n",
            "Collecting dm-control>=1.0.3 (from d4rl)\n",
            "  Downloading dm_control-1.0.25-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from dm-control>=1.0.3->d4rl) (1.4.0)\n",
            "Collecting dm-env (from dm-control>=1.0.3->d4rl)\n",
            "  Downloading dm_env-1.6-py3-none-any.whl.metadata (966 bytes)\n",
            "Requirement already satisfied: dm-tree!=0.1.2 in /usr/local/lib/python3.10/dist-packages (from dm-control>=1.0.3->d4rl) (0.1.8)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from dm-control>=1.0.3->d4rl) (2.8.0)\n",
            "Collecting labmaze (from dm-control>=1.0.3->d4rl)\n",
            "  Downloading labmaze-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (278 bytes)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from dm-control>=1.0.3->d4rl) (5.3.0)\n",
            "Collecting mujoco>=3.2.5 (from dm-control>=1.0.3->d4rl)\n",
            "  Downloading mujoco-3.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.19.4 in /usr/local/lib/python3.10/dist-packages (from dm-control>=1.0.3->d4rl) (4.25.5)\n",
            "Requirement already satisfied: pyopengl>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from dm-control>=1.0.3->d4rl) (3.1.7)\n",
            "Requirement already satisfied: pyparsing>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dm-control>=1.0.3->d4rl) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dm-control>=1.0.3->d4rl) (2.32.3)\n",
            "Requirement already satisfied: setuptools!=50.0.0 in /usr/local/lib/python3.10/dist-packages (from dm-control>=1.0.3->d4rl) (75.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dm-control>=1.0.3->d4rl) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dm-control>=1.0.3->d4rl) (4.66.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<0.25.0->d4rl) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<0.25.0->d4rl) (0.0.8)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from mujoco-py->d4rl) (3.0.11)\n",
            "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from mujoco-py->d4rl) (2.36.0)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco-py->d4rl) (1.17.1)\n",
            "Collecting fasteners~=0.15 (from mujoco-py->d4rl)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco-py->d4rl) (2.22)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.1.2->mujoco-py->d4rl) (11.0.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=3.2.5->dm-control>=1.0.3->d4rl) (1.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dm-control>=1.0.3->d4rl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dm-control>=1.0.3->d4rl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dm-control>=1.0.3->d4rl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dm-control>=1.0.3->d4rl) (2024.8.30)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.2.5->dm-control>=1.0.3->d4rl) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.2.5->dm-control>=1.0.3->d4rl) (6.4.5)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.2.5->dm-control>=1.0.3->d4rl) (4.12.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.2.5->dm-control>=1.0.3->d4rl) (3.21.0)\n",
            "Downloading d4rl-1.1-py3-none-any.whl (26.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dm_control-1.0.25-py3-none-any.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading mujoco-3.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dm_env-1.6-py3-none-any.whl (26 kB)\n",
            "Downloading labmaze-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybullet, labmaze, fasteners, dm-env, mujoco-py, mujoco, dm-control, d4rl\n",
            "  Attempting uninstall: mujoco\n",
            "    Found existing installation: mujoco 2.3.3\n",
            "    Uninstalling mujoco-2.3.3:\n",
            "      Successfully uninstalled mujoco-2.3.3\n",
            "Successfully installed d4rl-1.1 dm-control-1.0.25 dm-env-1.6 fasteners-0.19 labmaze-1.0.6 mujoco-3.2.5 mujoco-py-2.1.2.14 pybullet-3.2.6\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Cloning into 'miniBET'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 69 (delta 23), reused 64 (delta 22), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (69/69), 33.27 KiB | 681.00 KiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n",
            "/content/miniBET\n",
            "Obtaining file:///content/miniBET\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch<3,>=1.6 in /usr/local/lib/python3.10/dist-packages (from behavior_transformer==0.1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.10/dist-packages (from behavior_transformer==0.1.0) (0.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from behavior_transformer==0.1.0) (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.6->behavior_transformer==0.1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.6->behavior_transformer==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.6->behavior_transformer==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.6->behavior_transformer==0.1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.6->behavior_transformer==0.1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.6->behavior_transformer==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=1.6->behavior_transformer==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.6->behavior_transformer==0.1.0) (3.0.2)\n",
            "Installing collected packages: behavior_transformer\n",
            "  Running setup.py develop for behavior_transformer\n",
            "Successfully installed behavior_transformer-0.1.0\n",
            "/content/miniBET/examples\n",
            "Collecting d4rl (from -r requirements-dev.txt (line 4))\n",
            "  Cloning https://github.com/Farama-Foundation/d4rl (to revision 71a9549f2091accff93eeff68f1f3ab2c0e0a288) to /tmp/pip-install-omwrjsam/d4rl_0d8db268402b48218b594cf3797f53f8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/d4rl /tmp/pip-install-omwrjsam/d4rl_0d8db268402b48218b594cf3797f53f8\n",
            "  Running command git rev-parse -q --verify 'sha^71a9549f2091accff93eeff68f1f3ab2c0e0a288'\n",
            "  Running command git fetch -q https://github.com/Farama-Foundation/d4rl 71a9549f2091accff93eeff68f1f3ab2c0e0a288\n",
            "  Running command git checkout -q 71a9549f2091accff93eeff68f1f3ab2c0e0a288\n",
            "  Resolved https://github.com/Farama-Foundation/d4rl to commit 71a9549f2091accff93eeff68f1f3ab2c0e0a288\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hydra-core==1.3.1 (from -r requirements-dev.txt (line 1))\n",
            "  Downloading hydra_core-1.3.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting omegaconf==2.3.0 (from -r requirements-dev.txt (line 2))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting gym==0.21.0 (from -r requirements-dev.txt (line 3))\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
            "/usr/local/lib/python3.10/dist-packages/Cython/Distutils/old_build_ext.py:14: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n",
            "\n",
            "You appear to be missing MuJoCo.  We expected to find the file here: /root/.mujoco/mujoco210\n",
            "\n",
            "This package only provides python bindings, the library must be installed separately.\n",
            "\n",
            "Please follow the instructions on the README to install MuJoCo\n",
            "\n",
            "    https://github.com/openai/mujoco-py#install-mujoco\n",
            "\n",
            "Which can be downloaded from the website\n",
            "\n",
            "    https://www.roboti.us/index.html\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "\nYou appear to be missing MuJoCo.  We expected to find the file here: /root/.mujoco/mujoco210\n\nThis package only provides python bindings, the library must be installed separately.\n\nPlease follow the instructions on the README to install MuJoCo\n\n    https://github.com/openai/mujoco-py#install-mujoco\n\nWhich can be downloaded from the website\n\n    https://www.roboti.us/index.html\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3f84dd86af7f>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Download and set up the Franka Kitchen dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0md4rl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/d4rl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0md4rl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocomotion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0md4rl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhand_manipulation_suite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0md4rl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointmaze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/d4rl/locomotion/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0md4rl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocomotion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0md4rl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocomotion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmaze_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/d4rl/locomotion/ant.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mujoco_py/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!/usr/bin/env python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcymj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_mujoco_warnings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMujocoException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerated\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmjrenderpool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMjRenderPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmjviewer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMjViewer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMjViewerBasic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mujoco_py/builder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m \u001b[0mmujoco_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscover_mujoco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0mcymj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cython_ext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmujoco_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mujoco_py/utils.py\u001b[0m in \u001b[0;36mdiscover_mujoco\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMISSING_MUJOCO_MESSAGE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmujoco_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmujoco_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: \nYou appear to be missing MuJoCo.  We expected to find the file here: /root/.mujoco/mujoco210\n\nThis package only provides python bindings, the library must be installed separately.\n\nPlease follow the instructions on the README to install MuJoCo\n\n    https://github.com/openai/mujoco-py#install-mujoco\n\nWhich can be downloaded from the website\n\n    https://www.roboti.us/index.html\n"
          ]
        }
      ],
      "source": [
        "# Title: \"Franka Kitchen Environment Setup with miniBET in Colab\"\n",
        "# First, install required packages\n",
        "!pip install gym==0.24.1\n",
        "!pip install mujoco==2.3.3\n",
        "!pip install d4rl\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install wandb\n",
        "!git clone https://github.com/notmahi/miniBET.git\n",
        "%cd miniBET\n",
        "!pip install -e .\n",
        "%cd examples\n",
        "!pip install -r requirements-dev.txt\n",
        "\n",
        "# Download and set up the Franka Kitchen dataset\n",
        "import gym\n",
        "import d4rl\n",
        "import torch\n",
        "import numpy as np\n",
        "from behavior_transformer import BehaviorTransformer, GPT, GPTConfig\n",
        "\n",
        "# Create and test the environment\n",
        "env = gym.make('kitchen-complete-v0')\n",
        "print(\"Environment created successfully!\")\n",
        "\n",
        "# Basic environment info\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "\n",
        "# Setup miniBET model\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "goal_dim = obs_dim  # For conditional behavior\n",
        "K = 32\n",
        "T = 16\n",
        "batch_size = 256\n",
        "\n",
        "# Initialize the model\n",
        "cbet = BehaviorTransformer(\n",
        "    obs_dim=obs_dim,\n",
        "    act_dim=act_dim,\n",
        "    goal_dim=goal_dim,\n",
        "    gpt_model=GPT(\n",
        "        GPTConfig(\n",
        "            block_size=144,\n",
        "            input_dim=obs_dim,\n",
        "            n_layer=6,\n",
        "            n_head=8,\n",
        "            n_embd=256,\n",
        "        )\n",
        "    ),\n",
        "    n_clusters=K,\n",
        "    kmeans_fit_steps=5,\n",
        ")\n",
        "\n",
        "# Configure optimizer\n",
        "optimizer = cbet.configure_optimizers(\n",
        "    weight_decay=2e-4,\n",
        "    learning_rate=1e-5,\n",
        "    betas=[0.9, 0.999],\n",
        ")\n",
        "\n",
        "# Load some sample data from the environment\n",
        "dataset = env.get_dataset()\n",
        "print(\"\\nDataset keys:\", dataset.keys())\n",
        "print(\"Number of trajectories:\", len(dataset['observations']))\n",
        "\n",
        "# Create a simple training loop for testing\n",
        "def prepare_batch(dataset, batch_size, sequence_length):\n",
        "    idx = np.random.randint(0, len(dataset['observations']) - sequence_length, size=batch_size)\n",
        "    obs_seq = torch.tensor(np.stack([dataset['observations'][i:i+sequence_length] for i in idx]), dtype=torch.float32)\n",
        "    act_seq = torch.tensor(np.stack([dataset['actions'][i:i+sequence_length] for i in idx]), dtype=torch.float32)\n",
        "    goal_seq = obs_seq.clone()  # Using final states as goals for this example\n",
        "    return obs_seq, goal_seq, act_seq\n",
        "\n",
        "# Test training loop\n",
        "print(\"\\nTesting training loop...\")\n",
        "for i in range(3):\n",
        "    obs_seq, goal_seq, action_seq = prepare_batch(dataset, batch_size, T)\n",
        "    train_action, train_loss, train_loss_dict = cbet(obs_seq, goal_seq, action_seq)\n",
        "    print(f\"Iteration {i}, Loss: {train_loss.item():.4f}\")\n",
        "\n",
        "print(\"\\nSetup completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!apt-get update\n",
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common \\\n",
        "    patchelf\n",
        "\n",
        "# Install MuJoCo\n",
        "!wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
        "!mkdir -p ~/.mujoco\n",
        "!tar -xf mujoco.tar.gz -C ~/.mujoco\n",
        "!rm mujoco.tar.gz\n",
        "\n",
        "# Add MuJoCo to LD_LIBRARY_PATH\n",
        "import os\n",
        "os.environ['LD_LIBRARY_PATH'] = os.environ.get('LD_LIBRARY_PATH', '') + ':/root/.mujoco/mujoco210/bin'\n",
        "\n",
        "# Install Gymnasium-Robotics and dependencies\n",
        "!pip install gymnasium-robotics\n",
        "\n",
        "# Verify installation\n",
        "import gymnasium as gym\n",
        "import gymnasium_robotics\n",
        "\n",
        "# Test code to verify installation\n",
        "try:\n",
        "    # Try creating a Fetch environment\n",
        "    env = gym.make('FetchReach-v2', render_mode='human')\n",
        "    print(\"Successfully created FetchReach environment!\")\n",
        "\n",
        "    # Get basic environment info\n",
        "    print(\"\\nEnvironment Details:\")\n",
        "    print(f\"Action Space: {env.action_space}\")\n",
        "    print(f\"Observation Space: {env.observation_space}\")\n",
        "\n",
        "    # Close the environment\n",
        "    env.close()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred: {str(e)}\")\n",
        "    print(\"\\nTroubleshooting tips:\")\n",
        "    print(\"1. Make sure all dependencies are properly installed\")\n",
        "    print(\"2. Check if MuJoCo is correctly set up\")\n",
        "    print(\"3. Verify your Python version is compatible (3.8-3.11)\")\n",
        "\n",
        "print(\"\\nInstallation complete! You can now use Gymnasium-Robotics environments.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6-VY6ru7IBA",
        "outputId": "bbcc5781-980c-4c7b-9ccd-5090ef1ba578"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,172 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,618 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,223 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,734 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,452 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,502 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,323 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,512 kB]\n",
            "Fetched 23.9 MB in 3s (7,063 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "software-properties-common is already the newest version (0.99.22.9).\n",
            "The following additional packages will be installed:\n",
            "  libegl-dev libgl-dev libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev\n",
            "  libglvnd-dev libglx-dev libopengl-dev libosmesa6\n",
            "The following NEW packages will be installed:\n",
            "  libegl-dev libgl-dev libgl1-mesa-dev libgl1-mesa-glx libgles-dev libgles1 libglew-dev\n",
            "  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev libopengl-dev libosmesa6\n",
            "  libosmesa6-dev patchelf\n",
            "0 upgraded, 16 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,092 kB of archives.\n",
            "After this operation, 19.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.2 [6,842 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libglew-dev amd64 2.2.0-4 [287 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 patchelf amd64 0.14.3-1 [72.1 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6 amd64 23.2.1-1ubuntu3.1~22.04.2 [3,121 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6-dev amd64 23.2.1-1ubuntu3.1~22.04.2 [8,984 B]\n",
            "Fetched 4,092 kB in 2s (2,688 kB/s)\n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "(Reading database ... 123630 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../01-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../02-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "Preparing to unpack .../03-libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../04-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../05-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../06-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../09-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglu1-mesa-dev:amd64.\n",
            "Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglew-dev:amd64.\n",
            "Preparing to unpack .../12-libglew-dev_2.2.0-4_amd64.deb ...\n",
            "Unpacking libglew-dev:amd64 (2.2.0-4) ...\n",
            "Selecting previously unselected package patchelf.\n",
            "Preparing to unpack .../13-patchelf_0.14.3-1_amd64.deb ...\n",
            "Unpacking patchelf (0.14.3-1) ...\n",
            "Selecting previously unselected package libosmesa6:amd64.\n",
            "Preparing to unpack .../14-libosmesa6_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
            "Unpacking libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Selecting previously unselected package libosmesa6-dev:amd64.\n",
            "Preparing to unpack .../15-libosmesa6-dev_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
            "Unpacking libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up patchelf (0.14.3-1) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Setting up libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglew-dev:amd64 (2.2.0-4) ...\n",
            "Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "--2024-11-23 12:08:57--  https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz\n",
            "Resolving mujoco.org (mujoco.org)... 216.239.38.21, 216.239.34.21, 216.239.36.21, ...\n",
            "Connecting to mujoco.org (mujoco.org)|216.239.38.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/google-deepmind/mujoco/releases/download/2.1.0/mujoco210-linux-x86_64.tar.gz [following]\n",
            "--2024-11-23 12:08:57--  https://github.com/google-deepmind/mujoco/releases/download/2.1.0/mujoco210-linux-x86_64.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/400501136/1f51148e-4e64-4a12-a400-d6f1e21be444?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241123%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241123T120857Z&X-Amz-Expires=300&X-Amz-Signature=85e316be69387245140fca2e814d294da19c5b83da42523e04a86e2d028ea719&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmujoco210-linux-x86_64.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-11-23 12:08:57--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/400501136/1f51148e-4e64-4a12-a400-d6f1e21be444?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241123%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241123T120857Z&X-Amz-Expires=300&X-Amz-Signature=85e316be69387245140fca2e814d294da19c5b83da42523e04a86e2d028ea719&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmujoco210-linux-x86_64.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4385135 (4.2M) [application/octet-stream]\n",
            "Saving to: ‘mujoco.tar.gz’\n",
            "\n",
            "mujoco.tar.gz       100%[===================>]   4.18M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-11-23 12:08:58 (74.4 MB/s) - ‘mujoco.tar.gz’ saved [4385135/4385135]\n",
            "\n",
            "Collecting gymnasium-robotics\n",
            "  Downloading gymnasium_robotics-1.3.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting mujoco<3.2.0,>=2.2.0 (from gymnasium-robotics)\n",
            "  Downloading mujoco-3.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium-robotics) (1.26.4)\n",
            "Collecting gymnasium>=1.0.0 (from gymnasium-robotics)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting PettingZoo>=1.23.0 (from gymnasium-robotics)\n",
            "  Downloading pettingzoo-1.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: Jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium-robotics) (3.1.4)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from gymnasium-robotics) (2.36.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0->gymnasium-robotics) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0->gymnasium-robotics) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=1.0.0->gymnasium-robotics)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0.3->gymnasium-robotics) (3.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (1.10.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (2.8.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (3.1.7)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio->gymnasium-robotics) (11.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (3.21.0)\n",
            "Downloading gymnasium_robotics-1.3.1-py3-none-any.whl (26.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mujoco-3.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m847.8/847.8 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, PettingZoo, mujoco, gymnasium-robotics\n",
            "  Attempting uninstall: mujoco\n",
            "    Found existing installation: mujoco 3.2.5\n",
            "    Uninstalling mujoco-3.2.5:\n",
            "      Successfully uninstalled mujoco-3.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dm-control 1.0.25 requires mujoco>=3.2.5, but you have mujoco 3.1.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PettingZoo-1.24.3 farama-notifications-0.0.4 gymnasium-1.0.0 gymnasium-robotics-1.3.1 mujoco-3.1.6\n",
            "Error occurred: Environment version v2 for `FetchReach` is deprecated. Please use `FetchReach-v3` instead.\n",
            "\n",
            "Troubleshooting tips:\n",
            "1. Make sure all dependencies are properly installed\n",
            "2. Check if MuJoCo is correctly set up\n",
            "3. Verify your Python version is compatible (3.8-3.11)\n",
            "\n",
            "Installation complete! You can now use Gymnasium-Robotics environments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:517: DeprecationWarning: \u001b[33mWARN: The environment FetchReach-v2 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "import gymnasium_robotics\n",
        "\n",
        "gym.register_envs(gymnasium_robotics)\n",
        "\n",
        "env = gym.make('FrankaKitchen-v1', tasks_to_complete=['microwave', 'kettle'])"
      ],
      "metadata": {
        "id": "i5XgK16N6tSv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# Create and test Franka Kitchen environment\n",
        "try:\n",
        "    # Create the environment\n",
        "    env = gym.make('FrankaKitchen-v1')\n",
        "    print(\"Environment created successfully!\")\n",
        "\n",
        "    # Print environment information\n",
        "    print(\"\\nEnvironment Info:\")\n",
        "    print(f\"Action Space: {env.action_space}\")\n",
        "    print(f\"Observation Space: {env.observation_space}\")\n",
        "\n",
        "    # Run a test episode\n",
        "    print(\"\\nStarting test episode...\")\n",
        "    obs_dict, info = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "\n",
        "    # Print initial state information\n",
        "    print(\"\\nInitial State:\")\n",
        "    print(\"\\nObservation Components:\")\n",
        "    print(f\"Raw observation shape: {obs_dict['observation'].shape}\")\n",
        "    print(f\"Achieved goal states:\")\n",
        "    for key, value in obs_dict['achieved_goal'].items():\n",
        "        print(f\"  {key}: shape {value.shape}\")\n",
        "    print(f\"\\nDesired goal states:\")\n",
        "    for key, value in obs_dict['desired_goal'].items():\n",
        "        print(f\"  {key}: shape {value.shape}\")\n",
        "\n",
        "    while not done and steps < 200:  # Run for max 200 steps\n",
        "        # Sample a random action\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "        # Take a step in the environment\n",
        "        obs_dict, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "\n",
        "        # Print periodic updates\n",
        "        if steps % 20 == 0:\n",
        "            print(f\"\\nStep {steps}:\")\n",
        "            print(f\"Reward: {reward:.3f}\")\n",
        "            print(f\"Total Reward: {total_reward:.3f}\")\n",
        "            print(f\"Terminated: {terminated}\")\n",
        "\n",
        "            # Print some object states\n",
        "            print(\"\\nCurrent object states:\")\n",
        "            for obj, state in obs_dict['achieved_goal'].items():\n",
        "                print(f\"  {obj}: {state}\")\n",
        "\n",
        "    print(f\"\\nEpisode Summary:\")\n",
        "    print(f\"Total steps: {steps}\")\n",
        "    print(f\"Final reward: {total_reward:.3f}\")\n",
        "\n",
        "    # Close the environment\n",
        "    env.close()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "print(\"\\nTest complete!\")\n",
        "\n",
        "# Print detailed explanation of observation space\n",
        "print(\"\\nObservation Space Details:\")\n",
        "print(\"\\nObjects and their state dimensions:\")\n",
        "print(\"- Microwave: 1 dimension (door angle)\")\n",
        "print(\"- Kettle: 7 dimensions (position [3], orientation [4])\")\n",
        "print(\"- Bottom Burner: 2 dimensions (position [2])\")\n",
        "print(\"- Top Burner: 2 dimensions (position [2])\")\n",
        "print(\"- Light Switch: 2 dimensions (position [2])\")\n",
        "print(\"- Slide Cabinet: 1 dimension (position)\")\n",
        "print(\"- Hinge Cabinet: 2 dimensions (position [2])\")\n",
        "\n",
        "print(\"\\nAction Space Details:\")\n",
        "print(\"9 dimensions corresponding to Franka arm joint positions:\")\n",
        "print(\"1-7: 7 main arm joints\")\n",
        "print(\"8-9: 2 finger joints for the gripper\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQlGq7OQ77BD",
        "outputId": "76282864-b9b7-4441-d13e-2c74e3183081"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment created successfully!\n",
            "\n",
            "Environment Info:\n",
            "Action Space: Box(-1.0, 1.0, (9,), float64)\n",
            "Observation Space: Dict('achieved_goal': Dict('bottom burner': Box(-inf, inf, (2,), float64), 'hinge cabinet': Box(-inf, inf, (2,), float64), 'kettle': Box(-inf, inf, (7,), float64), 'light switch': Box(-inf, inf, (2,), float64), 'microwave': Box(-inf, inf, (1,), float64), 'slide cabinet': Box(-inf, inf, (1,), float64), 'top burner': Box(-inf, inf, (2,), float64)), 'desired_goal': Dict('bottom burner': Box(-inf, inf, (2,), float64), 'hinge cabinet': Box(-inf, inf, (2,), float64), 'kettle': Box(-inf, inf, (7,), float64), 'light switch': Box(-inf, inf, (2,), float64), 'microwave': Box(-inf, inf, (1,), float64), 'slide cabinet': Box(-inf, inf, (1,), float64), 'top burner': Box(-inf, inf, (2,), float64)), 'observation': Box(-inf, inf, (59,), float64))\n",
            "\n",
            "Starting test episode...\n",
            "\n",
            "Initial State:\n",
            "\n",
            "Observation Components:\n",
            "Raw observation shape: (59,)\n",
            "Achieved goal states:\n",
            "  bottom burner: shape (2,)\n",
            "  top burner: shape (2,)\n",
            "  light switch: shape (2,)\n",
            "  slide cabinet: shape (1,)\n",
            "  hinge cabinet: shape (2,)\n",
            "  microwave: shape (1,)\n",
            "  kettle: shape (7,)\n",
            "\n",
            "Desired goal states:\n",
            "  bottom burner: shape (2,)\n",
            "  top burner: shape (2,)\n",
            "  light switch: shape (2,)\n",
            "  slide cabinet: shape (1,)\n",
            "  hinge cabinet: shape (2,)\n",
            "  microwave: shape (1,)\n",
            "  kettle: shape (7,)\n",
            "\n",
            "Step 20:\n",
            "Reward: 0.000\n",
            "Total Reward: 0.000\n",
            "Terminated: False\n",
            "\n",
            "Current object states:\n",
            "  bottom burner: [2.45777456e-05 2.95589256e-07]\n",
            "  top burner: [2.45777655e-05 2.95589390e-07]\n",
            "  light switch: [2.16196259e-05 5.08073674e-06]\n",
            "  slide cabinet: [-2.15481874e-13]\n",
            "  hinge cabinet: [-6.44129196e-03 -7.63521757e-13]\n",
            "  microwave: [5.16345988e-13]\n",
            "  kettle: [-2.69463161e-01  3.50370179e-01  1.61936860e+00  9.99999955e-01\n",
            "  2.05984536e-08  7.04516396e-05 -2.91673863e-04]\n",
            "\n",
            "Step 40:\n",
            "Reward: 0.000\n",
            "Total Reward: 0.000\n",
            "Terminated: False\n",
            "\n",
            "Current object states:\n",
            "  bottom burner: [2.45777415e-05 2.95589228e-07]\n",
            "  top burner: [2.45777415e-05 2.95589228e-07]\n",
            "  light switch: [2.16196254e-05 5.08073670e-06]\n",
            "  slide cabinet: [-9.64543356e-23]\n",
            "  hinge cabinet: [-6.44129196e-03 -5.08860285e-22]\n",
            "  microwave: [3.16418148e-22]\n",
            "  kettle: [-2.69486722e-01  3.50370195e-01  1.61936225e+00  9.99999954e-01\n",
            " -2.58265030e-08 -8.85728042e-05 -2.91289677e-04]\n",
            "\n",
            "Step 60:\n",
            "Reward: 0.000\n",
            "Total Reward: 0.000\n",
            "Terminated: False\n",
            "\n",
            "Current object states:\n",
            "  bottom burner: [2.45777415e-05 2.95589228e-07]\n",
            "  top burner: [2.45777415e-05 2.95589228e-07]\n",
            "  light switch: [2.16196254e-05 5.08073670e-06]\n",
            "  slide cabinet: [-4.31750415e-32]\n",
            "  hinge cabinet: [-6.44129196e-03 -3.39137408e-31]\n",
            "  microwave: [1.93901855e-31]\n",
            "  kettle: [-2.69519728e-01  3.50370215e-01  1.61938363e+00  9.99999953e-01\n",
            "  2.96218607e-08  1.01550003e-04 -2.90906687e-04]\n",
            "\n",
            "Step 80:\n",
            "Reward: 0.000\n",
            "Total Reward: 0.000\n",
            "Terminated: False\n",
            "\n",
            "Current object states:\n",
            "  bottom burner: [2.45777415e-05 2.95589227e-07]\n",
            "  top burner: [2.45777415e-05 2.95589228e-07]\n",
            "  light switch: [2.16196254e-05 5.08073670e-06]\n",
            "  slide cabinet: [-1.932608e-41]\n",
            "  hinge cabinet: [-6.44129196e-03 -2.26023105e-40]\n",
            "  microwave: [1.18823555e-40]\n",
            "  kettle: [-2.69545830e-01  3.50370232e-01  1.61934906e+00  9.99999958e-01\n",
            "  5.63919527e-09  1.93254105e-05 -2.90522817e-04]\n",
            "\n",
            "Step 100:\n",
            "Reward: 0.000\n",
            "Total Reward: 0.000\n",
            "Terminated: False\n",
            "\n",
            "Current object states:\n",
            "  bottom burner: [2.45777415e-05 2.95589227e-07]\n",
            "  top burner: [2.45777415e-05 2.95589227e-07]\n",
            "  light switch: [2.16196254e-05 5.08073670e-06]\n",
            "  slide cabinet: [-8.65077035e-51]\n",
            "  hinge cabinet: [-6.44129196e-03 -1.50636417e-49]\n",
            "  microwave: [7.28153798e-50]\n",
            "  kettle: [-2.69572973e-01  3.50370249e-01  1.61938255e+00  9.99999958e-01\n",
            "  2.90500453e-09  9.89041327e-06 -2.90141176e-04]\n",
            "\n",
            "Step 120:\n",
            "Reward: 0.000\n",
            "Total Reward: 0.000\n",
            "Terminated: False\n",
            "\n",
            "Current object states:\n",
            "  bottom burner: [2.45777415e-05 2.95589228e-07]\n",
            "  top burner: [2.45777415e-05 2.95589228e-07]\n",
            "  light switch: [2.16196254e-05 5.08073670e-06]\n",
            "  slide cabinet: [-3.87227144e-60]\n",
            "  hinge cabinet: [-6.44129196e-03 -1.00393852e-58]\n",
            "  microwave: [4.46214519e-59]\n",
            "  kettle: [-2.69602294e-01  3.50370268e-01  1.61935449e+00  9.99999958e-01\n",
            "  8.34046098e-09  2.87037883e-05 -2.89758448e-04]\n",
            "\n",
            "Step 140:\n",
            "Reward: 0.000\n",
            "Total Reward: 0.000\n",
            "Terminated: False\n",
            "\n",
            "Current object states:\n",
            "  bottom burner: [2.45777415e-05 2.95589228e-07]\n",
            "  top burner: [2.45777415e-05 2.95589228e-07]\n",
            "  light switch: [2.16196254e-05 5.08073670e-06]\n",
            "  slide cabinet: [-1.73331223e-69]\n",
            "  hinge cabinet: [-6.44129196e-03 -6.69089566e-68]\n",
            "  microwave: [2.73441404e-68]\n",
            "  kettle: [-2.69625701e-01  3.50370283e-01  1.61937226e+00  9.99999952e-01\n",
            " -3.12002734e-08 -1.07761177e-04 -2.89377389e-04]\n",
            "\n",
            "Step 160:\n",
            "Reward: 0.000\n",
            "Total Reward: 0.000\n",
            "Terminated: False\n",
            "\n",
            "Current object states:\n",
            "  bottom burner: [2.45777415e-05 2.95589228e-07]\n",
            "  top burner: [2.45777415e-05 2.95589228e-07]\n",
            "  light switch: [2.16196254e-05 5.08073670e-06]\n",
            "  slide cabinet: [-7.7586795e-79]\n",
            "  hinge cabinet: [-6.44129196e-03 -4.45924565e-77]\n",
            "  microwave: [1.67565595e-77]\n",
            "  kettle: [-2.69659453e-01  3.50370304e-01  1.61936205e+00  9.99999955e-01\n",
            "  2.35143999e-08  8.11495452e-05 -2.88996124e-04]\n",
            "\n",
            "Step 180:\n",
            "Reward: 0.000\n",
            "Total Reward: 0.000\n",
            "Terminated: False\n",
            "\n",
            "Current object states:\n",
            "  bottom burner: [2.45777415e-05 2.95589228e-07]\n",
            "  top burner: [2.45777415e-05 2.95589228e-07]\n",
            "  light switch: [2.16196254e-05 5.08073670e-06]\n",
            "  slide cabinet: [-3.47295233e-88]\n",
            "  hinge cabinet: [-6.44129196e-03 -2.97192974e-86]\n",
            "  microwave: [1.02684627e-86]\n",
            "  kettle: [-2.69683549e-01  3.50370320e-01  1.61936314e+00  9.99999957e-01\n",
            " -1.62865449e-08 -5.64026399e-05 -2.88615691e-04]\n",
            "\n",
            "Step 200:\n",
            "Reward: 0.000\n",
            "Total Reward: 0.000\n",
            "Terminated: False\n",
            "\n",
            "Current object states:\n",
            "  bottom burner: [2.45777415e-05 2.95589227e-07]\n",
            "  top burner: [2.45777415e-05 2.95589228e-07]\n",
            "  light switch: [2.16196254e-05 5.08073670e-06]\n",
            "  slide cabinet: [-1.55456839e-97]\n",
            "  hinge cabinet: [-6.44129196e-03 -1.98068621e-95]\n",
            "  microwave: [6.29254037e-96]\n",
            "  kettle: [-2.69715582e-01  3.50370339e-01  1.61937913e+00  9.99999955e-01\n",
            "  2.43707676e-08  8.43451866e-05 -2.88236087e-04]\n",
            "\n",
            "Episode Summary:\n",
            "Total steps: 200\n",
            "Final reward: 0.000\n",
            "\n",
            "Test complete!\n",
            "\n",
            "Observation Space Details:\n",
            "\n",
            "Objects and their state dimensions:\n",
            "- Microwave: 1 dimension (door angle)\n",
            "- Kettle: 7 dimensions (position [3], orientation [4])\n",
            "- Bottom Burner: 2 dimensions (position [2])\n",
            "- Top Burner: 2 dimensions (position [2])\n",
            "- Light Switch: 2 dimensions (position [2])\n",
            "- Slide Cabinet: 1 dimension (position)\n",
            "- Hinge Cabinet: 2 dimensions (position [2])\n",
            "\n",
            "Action Space Details:\n",
            "9 dimensions corresponding to Franka arm joint positions:\n",
            "1-7: 7 main arm joints\n",
            "8-9: 2 finger joints for the gripper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install hydra-core==1.3.1 omegaconf==2.3.0\n",
        "!pip install h5py\n",
        "!git clone https://github.com/notmahi/miniBET.git\n",
        "%cd miniBET\n",
        "!pip install --upgrade .\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from behavior_transformer import BehaviorTransformer, GPT, GPTConfig\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e45fg9u38JPW",
        "outputId": "56ed9a4c-3046-4353-d297-77a7b0062cab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hydra-core==1.3.1\n",
            "  Using cached hydra_core-1.3.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting omegaconf==2.3.0\n",
            "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core==1.3.1)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core==1.3.1) (24.2)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.3.0) (6.0.2)\n",
            "Downloading hydra_core-1.3.1-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=e1b4a950f6efe0da4b8bd47ff380f8d59066bf44345cf63177952bb343523205\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf, hydra-core\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.1 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "162110c6bba144ed938708b32641c672"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.12.1)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.26.4)\n",
            "Cloning into 'miniBET'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 69 (delta 23), reused 64 (delta 22), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (69/69), 33.27 KiB | 6.65 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n",
            "/content/miniBET/examples/miniBET/miniBET\n",
            "Processing /content/miniBET/examples/miniBET/miniBET\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch<3,>=1.6 in /usr/local/lib/python3.10/dist-packages (from behavior_transformer==0.1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.10/dist-packages (from behavior_transformer==0.1.0) (0.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from behavior_transformer==0.1.0) (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.6->behavior_transformer==0.1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.6->behavior_transformer==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.6->behavior_transformer==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.6->behavior_transformer==0.1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.6->behavior_transformer==0.1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.6->behavior_transformer==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=1.6->behavior_transformer==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.6->behavior_transformer==0.1.0) (3.0.2)\n",
            "Building wheels for collected packages: behavior_transformer\n",
            "  Building wheel for behavior_transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for behavior_transformer: filename=behavior_transformer-0.1.0-py3-none-any.whl size=13746 sha256=ae3c9617acf2e558e03250b84e2104221a713f06be110cfbaa411989f9fdca4f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d5dttsxt/wheels/e7/85/43/71d1b93dfe6de04bac306d9cdd3a119f83e49e1dad4e25c107\n",
            "Successfully built behavior_transformer\n",
            "Installing collected packages: behavior_transformer\n",
            "  Attempting uninstall: behavior_transformer\n",
            "    Found existing installation: behavior_transformer 0.1.0\n",
            "    Uninstalling behavior_transformer-0.1.0:\n",
            "      Successfully uninstalled behavior_transformer-0.1.0\n",
            "Successfully installed behavior_transformer-0.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "behavior_transformer"
                ]
              },
              "id": "d21d009d87984c18a908ba9d5dc5dae6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment created successfully!\n",
            "Initializing BehaviorTransformer...\n",
            "number of parameters: 4.86M\n",
            "Collecting demonstration data...\n",
            "Data shapes:\n",
            "Observations: torch.Size([2, 16, 76])\n",
            "Actions: torch.Size([2, 16, 9])\n",
            "Goals: torch.Size([2, 16, 17])\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 1. Expected size 17 but got size 76 for tensor number 1 in the list.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fa4315f8c04e>\u001b[0m in \u001b[0;36m<cell line: 181>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-fa4315f8c04e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mtrain_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Step {i+1} - Training Loss: {train_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Evaluation phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/miniBET/examples/miniBET/behavior_transformer/bet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs_seq, goal_seq, action_seq)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_kmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     def _fit_kmeans(\n",
            "\u001b[0;32m/content/miniBET/examples/miniBET/behavior_transformer/bet.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, obs_seq, goal_seq, action_seq)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mgpt_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cbet_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGOAL_SPEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mgpt_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgoal_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cbet_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGOAL_SPEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mgpt_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgoal_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 17 but got size 76 for tensor number 1 in the list."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from behavior_transformer import BehaviorTransformer, GPT, GPTConfig\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'obs_dim': 59,    # Observation dimension\n",
        "    'act_dim': 9,     # Action dimension\n",
        "    'goal_dim': 59,   # Goal dimension\n",
        "    'K': 32,          # Number of clusters\n",
        "    'T': 16,          # Sequence length\n",
        "    'batch_size': 32, # Batch size\n",
        "    'training_steps': 100,\n",
        "    'block_size': 144,\n",
        "    'n_layer': 6,\n",
        "    'n_head': 8,\n",
        "    'n_embd': 256,\n",
        "}\n",
        "\n",
        "def create_synthetic_data(batch_size, seq_length, obs_dim, act_dim, goal_dim):\n",
        "    \"\"\"Create synthetic data for training\"\"\"\n",
        "    obs_seq = torch.randn(batch_size, seq_length, obs_dim)\n",
        "    goal_seq = torch.randn(batch_size, seq_length, goal_dim)\n",
        "    action_seq = torch.randn(batch_size, seq_length, act_dim)\n",
        "    return obs_seq, goal_seq, action_seq\n",
        "\n",
        "def format_loss_dict(loss_dict):\n",
        "    \"\"\"Format loss dictionary values properly\"\"\"\n",
        "    formatted = {}\n",
        "    for k, v in loss_dict.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            formatted[k] = v.item()\n",
        "        else:\n",
        "            formatted[k] = v\n",
        "    return formatted\n",
        "\n",
        "def main():\n",
        "    print(\"Initializing BehaviorTransformer...\")\n",
        "\n",
        "    # Create model\n",
        "    cbet = BehaviorTransformer(\n",
        "        obs_dim=CONFIG['obs_dim'],\n",
        "        act_dim=CONFIG['act_dim'],\n",
        "        goal_dim=CONFIG['goal_dim'],\n",
        "        gpt_model=GPT(\n",
        "            GPTConfig(\n",
        "                block_size=CONFIG['block_size'],\n",
        "                input_dim=CONFIG['obs_dim'],\n",
        "                n_layer=CONFIG['n_layer'],\n",
        "                n_head=CONFIG['n_head'],\n",
        "                n_embd=CONFIG['n_embd'],\n",
        "            )\n",
        "        ),\n",
        "        n_clusters=CONFIG['K'],\n",
        "        kmeans_fit_steps=5,\n",
        "    )\n",
        "\n",
        "    # Configure optimizer\n",
        "    optimizer = cbet.configure_optimizers(\n",
        "        weight_decay=2e-4,\n",
        "        learning_rate=1e-5,\n",
        "        betas=[0.9, 0.999],\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    # Training loop\n",
        "    for step in range(CONFIG['training_steps']):\n",
        "        # Generate synthetic data\n",
        "        obs_seq, goal_seq, action_seq = create_synthetic_data(\n",
        "            CONFIG['batch_size'],\n",
        "            CONFIG['T'],\n",
        "            CONFIG['obs_dim'],\n",
        "            CONFIG['act_dim'],\n",
        "            CONFIG['goal_dim']\n",
        "        )\n",
        "\n",
        "        # Training step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step < CONFIG['training_steps'] - 10:  # Training phase\n",
        "            pred_action, loss, loss_dict = cbet(obs_seq, goal_seq, action_seq)\n",
        "\n",
        "            if isinstance(loss, torch.Tensor):\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                loss_value = loss.item()\n",
        "            else:\n",
        "                loss_value = loss\n",
        "\n",
        "            if step % 10 == 0:\n",
        "                print(f\"\\nStep {step + 1}/{CONFIG['training_steps']}\")\n",
        "                print(f\"Training Loss: {loss_value:.4f}\")\n",
        "                if loss_dict is not None:\n",
        "                    formatted_loss = format_loss_dict(loss_dict)\n",
        "                    print(\"Loss components:\", formatted_loss)\n",
        "                print(\"---\")\n",
        "\n",
        "        else:  # Evaluation phase\n",
        "            with torch.no_grad():\n",
        "                pred_action, loss, loss_dict = cbet(obs_seq, goal_seq, None)\n",
        "                loss_value = loss.item() if isinstance(loss, torch.Tensor) else loss\n",
        "                print(f\"\\nEvaluation Step {step + 1}\")\n",
        "                print(f\"Evaluation Loss: {loss_value:.4f}\")\n",
        "                if loss_dict is not None:\n",
        "                    formatted_loss = format_loss_dict(loss_dict)\n",
        "                    print(\"Loss components:\", formatted_loss)\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "    print(\"Final model parameters:\", sum(p.numel() for p in cbet.parameters()))\n",
        "\n",
        "    # Save model\n",
        "    try:\n",
        "        torch.save(cbet.state_dict(), 'minibet_model.pth')\n",
        "        print(\"Model saved successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wtLHdXS-9L_M",
        "outputId": "c816f465-5284-4242-c73f-8e4218b165a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing BehaviorTransformer...\n",
            "number of parameters: 4.86M\n",
            "Starting training...\n",
            "\n",
            "Step 1/100\n",
            "Training Loss: 0.0000\n",
            "Loss components: {'classification_loss': 3.216613292694092, 'offset_loss': 1.0469565391540527, 'total_loss': 1050.173095703125}\n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "K-means clustering: 100%|██████████| 50/50 [00:00<00:00, 108.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 11/100\n",
            "Training Loss: 614.8088\n",
            "Loss components: {'classification_loss': 3.2579431533813477, 'offset_loss': 0.6115509271621704, 'total_loss': 614.808837890625}\n",
            "---\n",
            "\n",
            "Step 21/100\n",
            "Training Loss: 614.0748\n",
            "Loss components: {'classification_loss': 3.2736668586730957, 'offset_loss': 0.610801100730896, 'total_loss': 614.0747680664062}\n",
            "---\n",
            "\n",
            "Step 31/100\n",
            "Training Loss: 625.0662\n",
            "Loss components: {'classification_loss': 3.2745258808135986, 'offset_loss': 0.621791660785675, 'total_loss': 625.0662231445312}\n",
            "---\n",
            "\n",
            "Step 41/100\n",
            "Training Loss: 608.8214\n",
            "Loss components: {'classification_loss': 3.273685932159424, 'offset_loss': 0.6055477261543274, 'total_loss': 608.8214111328125}\n",
            "---\n",
            "\n",
            "Step 51/100\n",
            "Training Loss: 613.4265\n",
            "Loss components: {'classification_loss': 3.2872402667999268, 'offset_loss': 0.6101392507553101, 'total_loss': 613.4264526367188}\n",
            "---\n",
            "\n",
            "Step 61/100\n",
            "Training Loss: 601.4530\n",
            "Loss components: {'classification_loss': 3.2587456703186035, 'offset_loss': 0.5981943011283875, 'total_loss': 601.4530029296875}\n",
            "---\n",
            "\n",
            "Step 71/100\n",
            "Training Loss: 601.3088\n",
            "Loss components: {'classification_loss': 3.263559341430664, 'offset_loss': 0.598045289516449, 'total_loss': 601.308837890625}\n",
            "---\n",
            "\n",
            "Step 81/100\n",
            "Training Loss: 607.2886\n",
            "Loss components: {'classification_loss': 3.2475545406341553, 'offset_loss': 0.6040410399436951, 'total_loss': 607.28857421875}\n",
            "---\n",
            "\n",
            "Evaluation Step 91\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported format string passed to NoneType.__format__",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-072fde3978a6>\u001b[0m in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-072fde3978a6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEvaluation Step {step + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Evaluation Loss: {loss_value:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mloss_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mformatted_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_loss_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to NoneType.__format__"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Install additional requirements\n",
        "%cd examples\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loRDc80z-MfN",
        "outputId": "45877cb2-22e3-4883-a183-17f112353e1d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'examples'\n",
            "/content/miniBET/examples/miniBET/miniBET/examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "# Extract the dataset from Drive\n",
        "!tar -xzf /content/drive/MyDrive/bet_data_release.tar.gz\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4vn9qNDDO6I",
        "outputId": "33d97f9d-9ee2-4bcf-a15a-9afe28e5ceb5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "tar: -: Not found in archive\n",
            "tar: Exiting with failure status due to previous errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##preprocessing"
      ],
      "metadata": {
        "id": "NpO7g-ArHTSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import einops\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "class RelayKitchenTrajectoryDataset(Dataset):\n",
        "    def __init__(self, data_directory, device=\"cpu\", onehot_goals=False):\n",
        "        print(f\"Loading data from {data_directory}\")\n",
        "        self.data_dir = Path(data_directory)\n",
        "\n",
        "        try:\n",
        "            # Load observations from observations_seq.npy (not all_observations.npy)\n",
        "            print(\"\\nLoading observations...\")\n",
        "            self.observations = torch.from_numpy(np.load(self.data_dir / \"observations_seq.npy\"))\n",
        "            print(f\"Observation shape: {self.observations.shape}\")\n",
        "\n",
        "            # Load actions\n",
        "            print(\"Loading actions...\")\n",
        "            self.actions = torch.from_numpy(np.load(self.data_dir / \"actions_seq.npy\"))\n",
        "            print(f\"Action shape: {self.actions.shape}\")\n",
        "\n",
        "            # Load masks\n",
        "            print(\"Loading masks...\")\n",
        "            self.masks = torch.from_numpy(np.load(self.data_dir / \"existence_mask.npy\"))\n",
        "            print(f\"Mask shape: {self.masks.shape}\")\n",
        "\n",
        "            # Since there's no goals file, we'll use observations as goals\n",
        "            print(\"Using observations as goals...\")\n",
        "            self.goals = self.observations.clone()\n",
        "\n",
        "            # Print shapes before any transformations\n",
        "            print(\"\\nInitial shapes:\")\n",
        "            print(f\"Observations: {self.observations.shape}\")\n",
        "            print(f\"Actions: {self.actions.shape}\")\n",
        "            print(f\"Masks: {self.masks.shape}\")\n",
        "            print(f\"Goals: {self.goals.shape}\")\n",
        "\n",
        "            # Ensure all data is in T x N x D format\n",
        "            if len(self.observations.shape) == 3 and self.observations.shape[0] < self.observations.shape[1]:\n",
        "                print(\"\\nTransposing data to correct format...\")\n",
        "                self.observations = self.observations.transpose(0, 1)\n",
        "                self.actions = self.actions.transpose(0, 1)\n",
        "                self.masks = self.masks.transpose(0, 1)\n",
        "                self.goals = self.goals.transpose(0, 1)\n",
        "\n",
        "            # Move to device and store in tensors\n",
        "            self.tensors = [\n",
        "                self.observations.to(device).float(),\n",
        "                self.actions.to(device).float()\n",
        "            ]\n",
        "            if onehot_goals:\n",
        "                self.tensors.append(self.goals.to(device).float())\n",
        "\n",
        "            print(\"\\nFinal data shapes:\")\n",
        "            print(f\"Observations: {self.observations.shape}\")\n",
        "            print(f\"Actions: {self.actions.shape}\")\n",
        "            print(f\"Masks: {self.masks.shape}\")\n",
        "            print(f\"Goals: {self.goals.shape}\")\n",
        "\n",
        "            print(\"\\nData loaded successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError during data loading: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_seq_length(self, idx):\n",
        "        try:\n",
        "            return int(self.masks[idx].sum().item())\n",
        "        except Exception as e:\n",
        "            print(f\"Error in get_seq_length for idx {idx}: {e}\")\n",
        "            print(f\"Masks shape: {self.masks.shape}\")\n",
        "            raise\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.observations.shape[0]  # Number of trajectories\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self):\n",
        "            raise IndexError(f\"Index {idx} out of bounds for dataset with size {len(self)}\")\n",
        "\n",
        "        try:\n",
        "            T = self.get_seq_length(idx)\n",
        "            return tuple(x[idx, :T] for x in self.tensors)\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting item {idx}: {e}\")\n",
        "            print(f\"Dataset length: {len(self)}\")\n",
        "            print(f\"Tensors shapes: {[t.shape for t in self.tensors]}\")\n",
        "            raise\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    data_dir = '/content/drive/MyDrive/franka/kitchen'\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    batch_size = 32\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        # Create dataset\n",
        "        dataset = RelayKitchenTrajectoryDataset(\n",
        "            data_directory=data_dir,\n",
        "            device=device,\n",
        "            onehot_goals=True\n",
        "        )\n",
        "\n",
        "        print(f\"\\nDataset size: {len(dataset)}\")\n",
        "\n",
        "        # Test single item access\n",
        "        print(\"\\nTesting single item access:\")\n",
        "        first_item = dataset[0]\n",
        "        print(\"First item shapes:\")\n",
        "        for i, item in enumerate(first_item):\n",
        "            print(f\"Item {i} shape: {item.shape}\")\n",
        "\n",
        "        # Create DataLoader with smaller batch size and error checking\n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        print(f\"\\nDataLoader created with {len(dataloader)} batches\")\n",
        "\n",
        "        # Test first batch\n",
        "        print(\"\\nTesting first batch:\")\n",
        "        for batch in dataloader:\n",
        "            print(\"Batch shapes:\")\n",
        "            for i, item in enumerate(batch):\n",
        "                print(f\"Item {i} shape: {item.shape}\")\n",
        "            break\n",
        "\n",
        "        return dataset, dataloader\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {e}\")\n",
        "        return None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset, dataloader = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOaO1_XCHU8i",
        "outputId": "50f26c32-61df-4de6-b0e4-0ba7b9972065"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading data from /content/drive/MyDrive/franka/kitchen\n",
            "\n",
            "Loading observations...\n",
            "Observation shape: torch.Size([409, 566, 60])\n",
            "Loading actions...\n",
            "Action shape: torch.Size([409, 566, 9])\n",
            "Loading masks...\n",
            "Mask shape: torch.Size([409, 566])\n",
            "Using observations as goals...\n",
            "\n",
            "Initial shapes:\n",
            "Observations: torch.Size([409, 566, 60])\n",
            "Actions: torch.Size([409, 566, 9])\n",
            "Masks: torch.Size([409, 566])\n",
            "Goals: torch.Size([409, 566, 60])\n",
            "\n",
            "Transposing data to correct format...\n",
            "\n",
            "Final data shapes:\n",
            "Observations: torch.Size([566, 409, 60])\n",
            "Actions: torch.Size([566, 409, 9])\n",
            "Masks: torch.Size([566, 409])\n",
            "Goals: torch.Size([566, 409, 60])\n",
            "\n",
            "Data loaded successfully!\n",
            "\n",
            "Dataset size: 566\n",
            "\n",
            "Testing single item access:\n",
            "First item shapes:\n",
            "Item 0 shape: torch.Size([189, 60])\n",
            "Item 1 shape: torch.Size([189, 9])\n",
            "Item 2 shape: torch.Size([189, 60])\n",
            "\n",
            "DataLoader created with 18 batches\n",
            "\n",
            "Testing first batch:\n",
            "\n",
            "Error: Caught RuntimeError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n",
            "    return self.collate_fn(data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n",
            "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n",
            "    return [\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n",
            "    collate(samples, collate_fn_map=collate_fn_map)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n",
            "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 271, in collate_tensor_fn\n",
            "    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n",
            "RuntimeError: Trying to resize storage that is not resizable\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import einops\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, TensorDataset, Subset\n",
        "from torch import default_generator, randperm\n",
        "from itertools import accumulate\n",
        "from typing import Any, Callable, List, Optional, Sequence\n",
        "import abc\n",
        "\n",
        "class TrajectoryDataset(Dataset, abc.ABC):\n",
        "    \"\"\"\n",
        "    A dataset containing trajectories.\n",
        "    TrajectoryDataset[i] returns: (observations, actions, mask)\n",
        "        observations: Tensor[T, ...], T frames of observations\n",
        "        actions: Tensor[T, ...], T frames of actions\n",
        "        mask: Tensor[T]: 0: invalid; 1: valid\n",
        "    \"\"\"\n",
        "    @abc.abstractmethod\n",
        "    def get_seq_length(self, idx):\n",
        "        \"\"\"Returns the length of the idx-th trajectory.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "class TrajectorySubset(TrajectoryDataset, Subset):\n",
        "    \"\"\"\n",
        "    Subset of a trajectory dataset at specified indices.\n",
        "    Args:\n",
        "        dataset (TrajectoryDataset): The whole Dataset\n",
        "        indices (sequence): Indices in the whole set selected for subset\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset: TrajectoryDataset, indices: Sequence[int]):\n",
        "        Subset.__init__(self, dataset, indices)\n",
        "\n",
        "    def get_seq_length(self, idx):\n",
        "        return self.dataset.get_seq_length(self.indices[idx])\n",
        "\n",
        "class RelayKitchenTrajectoryDataset(TensorDataset, TrajectoryDataset):\n",
        "    def __init__(self, data_directory, device=\"cpu\", onehot_goals=False):\n",
        "        data_directory = Path(data_directory)\n",
        "        print(f\"Loading data from {data_directory}\")\n",
        "\n",
        "        # Load data\n",
        "        observations = torch.from_numpy(np.load(data_directory / \"observations_seq.npy\"))\n",
        "        actions = torch.from_numpy(np.load(data_directory / \"actions_seq.npy\"))\n",
        "        masks = torch.from_numpy(np.load(data_directory / \"existence_mask.npy\"))\n",
        "\n",
        "        print(\"\\nOriginal shapes:\")\n",
        "        print(f\"Observations: {observations.shape}\")\n",
        "        print(f\"Actions: {actions.shape}\")\n",
        "        print(f\"Masks: {masks.shape}\")\n",
        "\n",
        "        # The current values are in shape T x N x Dim, move to N x T x Dim\n",
        "        observations, actions, masks = transpose_batch_timestep(observations, actions, masks)\n",
        "\n",
        "        print(\"\\nTransposed shapes:\")\n",
        "        print(f\"Observations: {observations.shape}\")\n",
        "        print(f\"Actions: {actions.shape}\")\n",
        "        print(f\"Masks: {masks.shape}\")\n",
        "\n",
        "        self.masks = masks\n",
        "        tensors = [observations, actions]\n",
        "        if onehot_goals:\n",
        "            try:\n",
        "                goals = torch.load(data_directory / \"onehot_goals.pth\")\n",
        "                goals = next(transpose_batch_timestep(goals))\n",
        "                tensors.append(goals)\n",
        "                print(f\"Goals shape: {goals.shape}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not load onehot goals: {e}\")\n",
        "                print(\"Using observations as goals...\")\n",
        "                tensors.append(observations.clone())\n",
        "\n",
        "        tensors = [t.to(device).float() for t in tensors]\n",
        "        TensorDataset.__init__(self, *tensors)\n",
        "        self.actions = self.tensors[1]\n",
        "\n",
        "    def get_seq_length(self, idx):\n",
        "        return int(self.masks[idx].sum().item())\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        T = self.masks[idx].sum().int().item()\n",
        "        return tuple(x[idx, :T] for x in self.tensors)\n",
        "\n",
        "class TrajectorySlicerDataset(TrajectoryDataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset: TrajectoryDataset,\n",
        "        window: int,\n",
        "        future_conditional: bool = False,\n",
        "        min_future_sep: int = 0,\n",
        "        future_seq_len: Optional[int] = None,\n",
        "        only_sample_tail: bool = False,\n",
        "        transform: Optional[Callable] = None,\n",
        "    ):\n",
        "        if future_conditional:\n",
        "            assert future_seq_len is not None, \"must specify a future_seq_len\"\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.window = window\n",
        "        self.future_conditional = future_conditional\n",
        "        self.min_future_sep = min_future_sep\n",
        "        self.future_seq_len = future_seq_len\n",
        "        self.only_sample_tail = only_sample_tail\n",
        "        self.transform = transform\n",
        "        self.slices = []\n",
        "\n",
        "        min_seq_length = np.inf\n",
        "        for i in range(len(self.dataset)):\n",
        "            T = self.dataset.get_seq_length(i)\n",
        "            min_seq_length = min(T, min_seq_length)\n",
        "            if T - window < 0:\n",
        "                print(f\"Ignored short sequence #{i}: len={T}, window={window}\")\n",
        "            else:\n",
        "                self.slices += [(i, start, start + window) for start in range(T - window)]\n",
        "\n",
        "        if min_seq_length < window:\n",
        "            print(f\"Ignored short sequences. To include all, set window <= {min_seq_length}.\")\n",
        "\n",
        "    def get_seq_length(self, idx: int) -> int:\n",
        "        if self.future_conditional:\n",
        "            return self.future_seq_len + self.window\n",
        "        else:\n",
        "            return self.window\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.slices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i, start, end = self.slices[idx]\n",
        "        values = [x[start:end] for x in self.dataset[i]]\n",
        "\n",
        "        if self.future_conditional:\n",
        "            valid_start_range = (\n",
        "                end + self.min_future_sep,\n",
        "                self.dataset.get_seq_length(i) - self.future_seq_len,\n",
        "            )\n",
        "            if valid_start_range[0] < valid_start_range[1]:\n",
        "                if self.only_sample_tail:\n",
        "                    future_obs = self.dataset[i][0][-self.future_seq_len:]\n",
        "                else:\n",
        "                    start = np.random.randint(*valid_start_range)\n",
        "                    end = start + self.future_seq_len\n",
        "                    future_obs = self.dataset[i][0][start:end]\n",
        "            else:\n",
        "                # zeros placeholder T x obs_dim\n",
        "                _, obs_dim = values[0].shape\n",
        "                future_obs = torch.zeros((self.future_seq_len, obs_dim))\n",
        "            values.append(future_obs)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            values = self.transform(values)\n",
        "        return tuple(values)\n",
        "\n",
        "def random_split_traj(\n",
        "    dataset: TrajectoryDataset,\n",
        "    lengths: Sequence[int],\n",
        "    generator: Optional[torch.Generator] = default_generator,\n",
        ") -> List[TrajectorySubset]:\n",
        "    if sum(lengths) != len(dataset):\n",
        "        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n",
        "\n",
        "    indices = randperm(sum(lengths), generator=generator).tolist()\n",
        "    return [\n",
        "        TrajectorySubset(dataset, indices[offset - length : offset])\n",
        "        for offset, length in zip(accumulate(lengths), lengths)\n",
        "    ]\n",
        "\n",
        "def split_traj_datasets(dataset, train_fraction=0.95, random_seed=42):\n",
        "    dataset_length = len(dataset)\n",
        "    lengths = [\n",
        "        int(train_fraction * dataset_length),\n",
        "        dataset_length - int(train_fraction * dataset_length),\n",
        "    ]\n",
        "    generator = torch.Generator().manual_seed(random_seed)\n",
        "    return random_split_traj(dataset, lengths, generator=generator)\n",
        "\n",
        "def get_train_val_sliced(\n",
        "    traj_dataset: TrajectoryDataset,\n",
        "    train_fraction: float = 0.9,\n",
        "    random_seed: int = 42,\n",
        "    window_size: int = 10,\n",
        "    future_conditional: bool = False,\n",
        "    min_future_sep: int = 0,\n",
        "    future_seq_len: Optional[int] = None,\n",
        "    only_sample_tail: bool = False,\n",
        "    transform: Optional[Callable[[Any], Any]] = None,\n",
        "):\n",
        "    train, val = split_traj_datasets(\n",
        "        traj_dataset,\n",
        "        train_fraction=train_fraction,\n",
        "        random_seed=random_seed,\n",
        "    )\n",
        "    traj_slicer_kwargs = {\n",
        "        \"window\": window_size,\n",
        "        \"future_conditional\": future_conditional,\n",
        "        \"min_future_sep\": min_future_sep,\n",
        "        \"future_seq_len\": future_seq_len,\n",
        "        \"only_sample_tail\": only_sample_tail,\n",
        "        \"transform\": transform,\n",
        "    }\n",
        "    train_slices = TrajectorySlicerDataset(train, **traj_slicer_kwargs)\n",
        "    val_slices = TrajectorySlicerDataset(val, **traj_slicer_kwargs)\n",
        "    return train_slices, val_slices\n",
        "\n",
        "def transpose_batch_timestep(*args):\n",
        "    return (einops.rearrange(arg, \"t n ... -> n t ...\") for arg in args)\n",
        "\n",
        "def run_kitchen_dataset():\n",
        "    try:\n",
        "        # Configuration\n",
        "        data_dir = \"/content/drive/MyDrive/franka/kitchen\"\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "        # Create base dataset\n",
        "        print(\"\\nCreating base dataset...\")\n",
        "        dataset = RelayKitchenTrajectoryDataset(\n",
        "            data_directory=data_dir,\n",
        "            device=device,\n",
        "            onehot_goals=True\n",
        "        )\n",
        "        print(f\"Base dataset created with size: {len(dataset)}\")\n",
        "\n",
        "        # Create train and validation datasets\n",
        "        print(\"\\nCreating train/val splits...\")\n",
        "        train_dataset, val_dataset = get_train_val_sliced(\n",
        "            dataset,\n",
        "            train_fraction=0.9,\n",
        "            random_seed=42,\n",
        "            window_size=16,\n",
        "            future_conditional=False\n",
        "        )\n",
        "\n",
        "        print(\"\\nDataset sizes:\")\n",
        "        print(f\"Full dataset: {len(dataset)}\")\n",
        "        print(f\"Training slices: {len(train_dataset)}\")\n",
        "        print(f\"Validation slices: {len(val_dataset)}\")\n",
        "\n",
        "        # Create dataloaders\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=32,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=32,\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Test batch loading\n",
        "        print(\"\\nTesting batch loading:\")\n",
        "        for batch in train_loader:\n",
        "            print(\"Batch shapes:\")\n",
        "            for i, item in enumerate(batch):\n",
        "                print(f\"Item {i} shape: {item.shape}\")\n",
        "            break\n",
        "\n",
        "        return dataset, train_dataset, val_dataset, train_loader, val_loader\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    result = run_kitchen_dataset()\n",
        "    if result:\n",
        "        dataset, train_dataset, val_dataset, train_loader, val_loader = result\n",
        "        print(\"\\nAll components created successfully!\")\n",
        "    else:\n",
        "        print(\"\\nFailed to create dataset components\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsNwtGeAKYLc",
        "outputId": "92df59d8-81ce-4db1-953b-01ff457724b9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "Creating base dataset...\n",
            "Loading data from /content/drive/MyDrive/franka/kitchen\n",
            "\n",
            "Original shapes:\n",
            "Observations: torch.Size([409, 566, 60])\n",
            "Actions: torch.Size([409, 566, 9])\n",
            "Masks: torch.Size([409, 566])\n",
            "\n",
            "Transposed shapes:\n",
            "Observations: torch.Size([566, 409, 60])\n",
            "Actions: torch.Size([566, 409, 9])\n",
            "Masks: torch.Size([566, 409])\n",
            "Warning: Could not load onehot goals: [Errno 2] No such file or directory: '/content/drive/MyDrive/franka/kitchen/onehot_goals.pth'\n",
            "Using observations as goals...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-a4ca5aa07ff6>:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  goals = torch.load(data_directory / \"onehot_goals.pth\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base dataset created with size: 566\n",
            "\n",
            "Creating train/val splits...\n",
            "\n",
            "Dataset sizes:\n",
            "Full dataset: 566\n",
            "Training slices: 107387\n",
            "Validation slices: 12242\n",
            "\n",
            "Testing batch loading:\n",
            "Batch shapes:\n",
            "Item 0 shape: torch.Size([32, 16, 60])\n",
            "Item 1 shape: torch.Size([32, 16, 9])\n",
            "Item 2 shape: torch.Size([32, 16, 60])\n",
            "\n",
            "All components created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##now train"
      ],
      "metadata": {
        "id": "crzx9XmDKuKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import einops\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, TensorDataset, Subset\n",
        "from torch import default_generator, randperm\n",
        "from itertools import accumulate\n",
        "from typing import Any, Callable, List, Optional, Sequence\n",
        "import abc\n",
        "from behavior_transformer import BehaviorTransformer, GPT, GPTConfig\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "\n",
        "class KitchenTrainer:\n",
        "    def __init__(self, config=None):\n",
        "        self.config = {\n",
        "            # Model parameters\n",
        "            'obs_dim': 60,\n",
        "            'act_dim': 9,\n",
        "            'goal_dim': 60,\n",
        "            'n_layer': 6,\n",
        "            'n_head': 8,\n",
        "            'n_embd': 256,\n",
        "            'block_size': 144,\n",
        "            'n_clusters': 32,\n",
        "\n",
        "            # Training parameters\n",
        "            'batch_size': 32,\n",
        "            'learning_rate': 1e-5,\n",
        "            'weight_decay': 2e-4,\n",
        "            'betas': [0.9, 0.999],\n",
        "            'num_epochs': 100,\n",
        "            'save_freq': 10,\n",
        "            'eval_freq': 5,\n",
        "\n",
        "            # Paths\n",
        "            'save_dir': '/content/drive/MyDrive/franka/kitchen/checkpoints',\n",
        "            'data_dir': '/content/drive/MyDrive/franka/kitchen',\n",
        "\n",
        "            # Device\n",
        "            'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "            # Wandb config\n",
        "            'use_wandb': True,\n",
        "            'wandb_project': 'kitchen-cbet',\n",
        "            'wandb_entity': None,  # Your wandb username\n",
        "            'experiment_name': 'kitchen-training'\n",
        "        }\n",
        "        if config:\n",
        "            self.config.update(config)\n",
        "\n",
        "        # Create save directory\n",
        "        os.makedirs(self.config['save_dir'], exist_ok=True)\n",
        "\n",
        "        # Initialize wandb\n",
        "        if self.config['use_wandb']:\n",
        "            wandb.init(\n",
        "                project=self.config['wandb_project'],\n",
        "                entity=self.config['wandb_entity'],\n",
        "                config=self.config,\n",
        "                name=self.config['experiment_name']\n",
        "            )\n",
        "\n",
        "    def create_model(self):\n",
        "        \"\"\"Create CBET model\"\"\"\n",
        "        model = BehaviorTransformer(\n",
        "            obs_dim=self.config['obs_dim'],\n",
        "            act_dim=self.config['act_dim'],\n",
        "            goal_dim=self.config['goal_dim'],\n",
        "            gpt_model=GPT(\n",
        "                GPTConfig(\n",
        "                    block_size=self.config['block_size'],\n",
        "                    input_dim=self.config['obs_dim'],\n",
        "                    n_layer=self.config['n_layer'],\n",
        "                    n_head=self.config['n_head'],\n",
        "                    n_embd=self.config['n_embd'],\n",
        "                )\n",
        "            ),\n",
        "            n_clusters=self.config['n_clusters'],\n",
        "            kmeans_fit_steps=5,\n",
        "        ).to(self.config['device'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save_checkpoint(self, model, optimizer, epoch, loss, path):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            'config': self.config\n",
        "        }, path)\n",
        "\n",
        "    def load_checkpoint(self, model, optimizer, path):\n",
        "        \"\"\"Load model checkpoint\"\"\"\n",
        "        checkpoint = torch.load(path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        return checkpoint['epoch'], checkpoint['loss']\n",
        "\n",
        "    def train(self, train_loader, val_loader, checkpoint_path=None):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        print(\"Initializing training...\")\n",
        "        print(f\"Using device: {self.config['device']}\")\n",
        "\n",
        "        # Create model\n",
        "        model = self.create_model()\n",
        "        print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "\n",
        "        # Create optimizer\n",
        "        optimizer = model.configure_optimizers(\n",
        "            weight_decay=self.config['weight_decay'],\n",
        "            learning_rate=self.config['learning_rate'],\n",
        "            betas=self.config['betas'],\n",
        "        )\n",
        "\n",
        "        # Load checkpoint if provided\n",
        "        start_epoch = 0\n",
        "        if checkpoint_path:\n",
        "            start_epoch, _ = self.load_checkpoint(model, optimizer, checkpoint_path)\n",
        "            print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
        "\n",
        "        print(\"Starting training...\")\n",
        "        for epoch in range(start_epoch, self.config['num_epochs']):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "            train_batches = 0\n",
        "\n",
        "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{self.config['num_epochs']}\")\n",
        "            for batch in progress_bar:\n",
        "                # Move batch to device\n",
        "                obs, acts, goals = [x.to(self.config['device']) for x in batch]\n",
        "\n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                pred_actions, loss, loss_dict = model(obs, goals, acts)\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update metrics\n",
        "                train_loss += loss.item()\n",
        "                train_batches += 1\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "                # Log to wandb\n",
        "                if self.config['use_wandb']:\n",
        "                    wandb.log({\n",
        "                        'train/loss': loss.item(),\n",
        "                        'train/epoch': epoch,\n",
        "                        **{f\"train/{k}\": v.item() if isinstance(v, torch.Tensor) else v\n",
        "                           for k, v in loss_dict.items()}\n",
        "                    })\n",
        "\n",
        "            avg_train_loss = train_loss / train_batches\n",
        "            print(f\"\\nEpoch {epoch+1} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "            # Evaluation phase\n",
        "            if (epoch + 1) % self.config['eval_freq'] == 0:\n",
        "                model.eval()\n",
        "                val_loss = 0\n",
        "                val_batches = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "                        obs, acts, goals = [x.to(self.config['device']) for x in batch]\n",
        "                        _, loss, loss_dict = model(obs, goals, acts)\n",
        "                        val_loss += loss.item()\n",
        "                        val_batches += 1\n",
        "\n",
        "                        if self.config['use_wandb']:\n",
        "                            wandb.log({\n",
        "                                'val/loss': loss.item(),\n",
        "                                'val/epoch': epoch,\n",
        "                                **{f\"val/{k}\": v.item() if isinstance(v, torch.Tensor) else v\n",
        "                                   for k, v in loss_dict.items()}\n",
        "                            })\n",
        "\n",
        "                avg_val_loss = val_loss / val_batches\n",
        "                print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "            # Save checkpoint\n",
        "            if (epoch + 1) % self.config['save_freq'] == 0:\n",
        "                save_path = Path(self.config['save_dir']) / f\"checkpoint_epoch_{epoch+1}.pt\"\n",
        "                self.save_checkpoint(model, optimizer, epoch, avg_train_loss, save_path)\n",
        "                print(f\"Checkpoint saved: {save_path}\")\n",
        "\n",
        "        # Save final model\n",
        "        final_path = Path(self.config['save_dir']) / \"final_model.pt\"\n",
        "        self.save_checkpoint(model, optimizer, self.config['num_epochs']-1, avg_train_loss, final_path)\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "        if self.config['use_wandb']:\n",
        "            wandb.finish()\n",
        "\n",
        "        return model\n",
        "\n",
        "def main():\n",
        "    # Create dataset\n",
        "\n",
        "    result = run_kitchen_dataset()\n",
        "\n",
        "    if not result:\n",
        "        print(\"Failed to create datasets\")\n",
        "        return\n",
        "\n",
        "    dataset, train_dataset, val_dataset, train_loader, val_loader = result\n",
        "\n",
        "    # Create trainer and train\n",
        "    trainer = KitchenTrainer()\n",
        "    model = trainer.train(train_loader, val_loader)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hv28hjk5KvR8",
        "outputId": "3687197b-1b77-449c-cd66-66278f3b2bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "Creating base dataset...\n",
            "Loading data from /content/drive/MyDrive/franka/kitchen\n",
            "\n",
            "Original shapes:\n",
            "Observations: torch.Size([409, 566, 60])\n",
            "Actions: torch.Size([409, 566, 9])\n",
            "Masks: torch.Size([409, 566])\n",
            "\n",
            "Transposed shapes:\n",
            "Observations: torch.Size([566, 409, 60])\n",
            "Actions: torch.Size([566, 409, 9])\n",
            "Masks: torch.Size([566, 409])\n",
            "Warning: Could not load onehot goals: [Errno 2] No such file or directory: '/content/drive/MyDrive/franka/kitchen/onehot_goals.pth'\n",
            "Using observations as goals...\n",
            "Base dataset created with size: 566\n",
            "\n",
            "Creating train/val splits...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-a4ca5aa07ff6>:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  goals = torch.load(data_directory / \"onehot_goals.pth\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset sizes:\n",
            "Full dataset: 566\n",
            "Training slices: 107387\n",
            "Validation slices: 12242\n",
            "\n",
            "Testing batch loading:\n",
            "Batch shapes:\n",
            "Item 0 shape: torch.Size([32, 16, 60])\n",
            "Item 1 shape: torch.Size([32, 16, 9])\n",
            "Item 2 shape: torch.Size([32, 16, 60])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/miniBET/examples/miniBET/miniBET/examples/wandb/run-20241123_131842-6vhceex9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hb12/kitchen-cbet/runs/6vhceex9' target=\"_blank\">kitchen-training</a></strong> to <a href='https://wandb.ai/hb12/kitchen-cbet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hb12/kitchen-cbet' target=\"_blank\">https://wandb.ai/hb12/kitchen-cbet</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hb12/kitchen-cbet/runs/6vhceex9' target=\"_blank\">https://wandb.ai/hb12/kitchen-cbet/runs/6vhceex9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing training...\n",
            "Using device: cpu\n",
            "number of parameters: 4.86M\n",
            "Model created with 4939328 parameters\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/100:   0%|          | 4/3356 [00:04<1:00:44,  1.09s/it, loss=0]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "K-means clustering:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "K-means clustering:  38%|███▊      | 19/50 [00:00<00:00, 187.59it/s]\u001b[A\n",
            "K-means clustering: 100%|██████████| 50/50 [00:00<00:00, 175.32it/s]\n",
            "Epoch 1/100:   6%|▌         | 198/3356 [03:08<54:16,  1.03s/it, loss=35.1]"
          ]
        }
      ]
    }
  ]
}